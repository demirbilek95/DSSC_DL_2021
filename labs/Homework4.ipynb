{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "virtual-charles",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!).\n",
    "The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aware-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "descending-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter\n",
    "from scripts import architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-recorder",
   "metadata": {},
   "source": [
    "- **Training time until convergence:** There seems to be a sweet spot. If the batch size is very small (e.g. 8), this time goes up. If the batch size is huge, it is also higher than the minimum.\n",
    "\n",
    "- **Training time per epoch:** Bigger computes faster (is efficient)\n",
    "\n",
    "- **Resulting model quality:** The lower the better due to better generalization (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, \n",
    "                                                            batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "jewish-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),       \n",
    "\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "remarkable-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, accuracy_meter):\n",
    "    for X, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = accuracy(y_hat, y)\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        accuracy_meter.update(val=acc, n=X.shape[0])\n",
    "        \n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs,lr_scheduler=None, epoch_start_scheduler=1):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, accuracy_meter)\n",
    "        # now with loss meter we can print both the cumulative value and the average value\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Accuracy: {accuracy_meter.avg}\")\n",
    "    # we also return the stats for the final epoch of training\n",
    "    \n",
    "        if lr_scheduler is not None:\n",
    "            if epoch >= epoch_start_scheduler:\n",
    "                lr_scheduler.step()\n",
    "    return loss_meter.sum, accuracy_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acute-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n",
      "Epoch 1 completed. Loss - total: 25049.58820915222 - average: 0.4174931368192037; Accuracy: 0.9212333333333333\n",
      "Epoch 2 completed. Loss - total: 7171.282043457031 - average: 0.11952136739095053; Accuracy: 0.9711166666666666\n",
      "Epoch 3 completed. Loss - total: 4859.559731960297 - average: 0.08099266219933828; Accuracy: 0.9783166666666666\n",
      "Epoch 4 completed. Loss - total: 3597.719836473465 - average: 0.05996199727455775; Accuracy: 0.9833\n",
      "Epoch 5 completed. Loss - total: 2773.6652530431747 - average: 0.046227754217386244; Accuracy: 0.9862\n",
      "Epoch 6 completed. Loss - total: 2214.2340394556522 - average: 0.036903900657594205; Accuracy: 0.9892333333333333\n",
      "Epoch 7 completed. Loss - total: 1082.5240494012833 - average: 0.018042067490021386; Accuracy: 0.9953\n",
      "Epoch 8 completed. Loss - total: 686.4664028435946 - average: 0.01144110671405991; Accuracy: 0.9979333333333333\n",
      "Epoch 9 completed. Loss - total: 572.324460029602 - average: 0.009538741000493367; Accuracy: 0.9984333333333333\n",
      "Epoch 10 completed. Loss - total: 498.8505711555481 - average: 0.008314176185925802; Accuracy: 0.9988166666666667\n",
      "Epoch 11 completed. Loss - total: 427.82574155926704 - average: 0.007130429025987784; Accuracy: 0.9990333333333333\n",
      "Epoch 12 completed. Loss - total: 366.51936864852905 - average: 0.006108656144142151; Accuracy: 0.9992833333333333\n",
      "Epoch 13 completed. Loss - total: 369.71970035135746 - average: 0.006161995005855958; Accuracy: 0.9993333333333333\n",
      "Epoch 14 completed. Loss - total: 355.0537326782942 - average: 0.005917562211304903; Accuracy: 0.9993333333333333\n",
      "Epoch 15 completed. Loss - total: 350.65192782878876 - average: 0.005844198797146479; Accuracy: 0.99935\n",
      "Epoch 16 completed. Loss - total: 342.17333447933197 - average: 0.005702888907988866; Accuracy: 0.9994333333333333\n",
      "Epoch 17 completed. Loss - total: 347.9535813629627 - average: 0.0057992263560493786; Accuracy: 0.99935\n",
      "Epoch 18 completed. Loss - total: 336.05787965655327 - average: 0.005600964660942554; Accuracy: 0.99945\n",
      "Epoch 19 completed. Loss - total: 332.50465420633554 - average: 0.005541744236772259; Accuracy: 0.9994833333333333\n",
      "Epoch 20 completed. Loss - total: 331.5814508795738 - average: 0.005526357514659564; Accuracy: 0.9994833333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(331.5814508795738, 0.9994833333333333)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "learn_rate = 0.1 # for SGD\n",
    "num_epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = MLP()\n",
    "adam = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(adam, step_size=5, gamma=.1)\n",
    "\n",
    "train_model(model, trainloader, loss_fn, adam, num_epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-testing",
   "metadata": {},
   "source": [
    "2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530).\n",
    "*Tip*: To permute the labels, act on the `trainset.targets` with an appropriate torch function.\n",
    "Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so: `trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`. You can now use this `DataLoader` inside the training function.\n",
    "Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "loaded-threshold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
