{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "virtual-charles",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!).\n",
    "The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aware-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "descending-vertex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff9a8175b50>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter\n",
    "from scripts import architectures\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-recorder",
   "metadata": {},
   "source": [
    "- **Training time until convergence:** There seems to be a sweet spot. If the batch size is very small (e.g. 8), this time goes up. If the batch size is huge, it is also higher than the minimum.\n",
    "\n",
    "- **Training time per epoch:** Bigger computes faster (is efficient)\n",
    "\n",
    "- **Resulting model quality:** The lower the better due to better generalization (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "educated-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, \n",
    "                                                            batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "jewish-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),       \n",
    "\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "remarkable-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, accuracy_meter):\n",
    "    for X, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = accuracy(y_hat, y)\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        accuracy_meter.update(val=acc, n=X.shape[0])\n",
    "        \n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs,lr_scheduler=None, epoch_start_scheduler=1):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, accuracy_meter)\n",
    "        # now with loss meter we can print both the cumulative value and the average value\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Accuracy: {accuracy_meter.avg}\")\n",
    "    # we also return the stats for the final epoch of training\n",
    "    \n",
    "        if lr_scheduler is not None:\n",
    "            if epoch >= epoch_start_scheduler:\n",
    "                lr_scheduler.step()\n",
    "    return loss_meter.sum, accuracy_meter.avg\n",
    "\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None):\n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acute-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n",
      "Epoch 1 completed. Loss - total: 25245.082615852356 - average: 0.4207513769308726; Accuracy: 0.91925\n",
      "Epoch 2 completed. Loss - total: 6942.592653989792 - average: 0.11570987756649653; Accuracy: 0.9726\n",
      "Epoch 3 completed. Loss - total: 4816.2511620521545 - average: 0.08027085270086924; Accuracy: 0.9780166666666666\n",
      "Epoch 4 completed. Loss - total: 3532.179852962494 - average: 0.05886966421604156; Accuracy: 0.9840166666666667\n",
      "Epoch 5 completed. Loss - total: 2648.7057873010635 - average: 0.04414509645501773; Accuracy: 0.9872833333333333\n",
      "Epoch 6 completed. Loss - total: 2216.1498260498047 - average: 0.03693583043416341; Accuracy: 0.9890333333333333\n",
      "Epoch 7 completed. Loss - total: 1064.156699001789 - average: 0.01773594498336315; Accuracy: 0.9952666666666666\n",
      "Epoch 8 completed. Loss - total: 618.9858611226082 - average: 0.010316431018710137; Accuracy: 0.9980833333333333\n",
      "Epoch 9 completed. Loss - total: 496.1282648444176 - average: 0.008268804414073625; Accuracy: 0.9987333333333334\n",
      "Epoch 10 completed. Loss - total: 430.5173544883728 - average: 0.00717528924147288; Accuracy: 0.9991\n",
      "Epoch 11 completed. Loss - total: 386.36289010196924 - average: 0.006439381501699488; Accuracy: 0.9992833333333333\n",
      "Epoch 12 completed. Loss - total: 324.4180707335472 - average: 0.0054069678455591205; Accuracy: 0.9995333333333334\n",
      "Epoch 13 completed. Loss - total: 331.08741933107376 - average: 0.005518123655517896; Accuracy: 0.9995833333333334\n",
      "Epoch 14 completed. Loss - total: 330.7752598673105 - average: 0.005512920997788509; Accuracy: 0.9995166666666667\n",
      "Epoch 15 completed. Loss - total: 327.85258489847183 - average: 0.005464209748307864; Accuracy: 0.99955\n",
      "Epoch 16 completed. Loss - total: 309.5578416585922 - average: 0.005159297360976537; Accuracy: 0.9995666666666667\n",
      "Epoch 17 completed. Loss - total: 303.6541501060128 - average: 0.005060902501766881; Accuracy: 0.9996333333333334\n",
      "Epoch 18 completed. Loss - total: 301.5906185656786 - average: 0.005026510309427977; Accuracy: 0.9996333333333334\n",
      "Epoch 19 completed. Loss - total: 304.2322975695133 - average: 0.005070538292825222; Accuracy: 0.9995833333333334\n",
      "Epoch 20 completed. Loss - total: 315.8218548297882 - average: 0.00526369758049647; Accuracy: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(315.8218548297882, 0.9995)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "learn_rate = 0.1 # for SGD\n",
    "num_epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = MLP()\n",
    "adam = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(adam, step_size=5, gamma=.1)\n",
    "\n",
    "train_model(model, trainloader, loss_fn, adam, num_epochs,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "united-collector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss -- - performance 0.9987166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0.9987166666666667)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-blocking",
   "metadata": {},
   "source": [
    "**Note: Since it is said that we can neglect the overfitting, none of the regularization techniques weren't used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-testing",
   "metadata": {},
   "source": [
    "2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530).\n",
    "*Tip*: To permute the labels, act on the `trainset.targets` with an appropriate torch function.\n",
    "Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so: `trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`. You can now use this `DataLoader` inside the training function.\n",
    "Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "compressed-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloaderToPerm, testloader, trainsetToPerm, testset = mnist.get_data(batch_size_train=1024, \n",
    "                                                            batch_size_test=1024)\n",
    "\n",
    "indexes = torch.randperm(trainsetToPerm.targets.shape[0])\n",
    "trainset_target_permuted = trainsetToPerm.targets[indexes]\n",
    "trainsetToPerm.targets = trainset_target_permuted\n",
    "\n",
    "trainloader_permuted = torch.utils.data.DataLoader(trainsetToPerm, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "elect-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1500),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=1500),\n",
    "            nn.Linear(1500, 750),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=750),\n",
    "            nn.Linear(750, 400),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_features=400),\n",
    "            nn.Linear(400, 10),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "vocational-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "Epoch 1 completed. Loss - total: 141442.35330963135 - average: 2.3573725551605222; Accuracy: 0.1004\n",
      "Epoch 2 completed. Loss - total: 138155.65364837646 - average: 2.302594227472941; Accuracy: 0.1223\n",
      "Epoch 3 completed. Loss - total: 137127.17332458496 - average: 2.2854528887430825; Accuracy: 0.13505\n",
      "Epoch 4 completed. Loss - total: 136427.59621429443 - average: 2.2737932702382406; Accuracy: 0.14665\n",
      "Epoch 5 completed. Loss - total: 135899.9328765869 - average: 2.2649988812764486; Accuracy: 0.15066666666666667\n",
      "Epoch 6 completed. Loss - total: 135113.18854522705 - average: 2.251886475753784; Accuracy: 0.16115\n",
      "Epoch 7 completed. Loss - total: 134509.1104888916 - average: 2.241818508148193; Accuracy: 0.16918333333333332\n",
      "Epoch 8 completed. Loss - total: 133746.01634979248 - average: 2.229100272496541; Accuracy: 0.17728333333333332\n",
      "Epoch 9 completed. Loss - total: 132826.2497024536 - average: 2.213770828374227; Accuracy: 0.18591666666666667\n",
      "Epoch 10 completed. Loss - total: 131879.5874710083 - average: 2.197993124516805; Accuracy: 0.20036666666666667\n",
      "Epoch 11 completed. Loss - total: 130874.05954742432 - average: 2.181234325790405; Accuracy: 0.20716666666666667\n",
      "Epoch 12 completed. Loss - total: 129623.61589050293 - average: 2.1603935981750486; Accuracy: 0.22001666666666667\n",
      "Epoch 13 completed. Loss - total: 128241.45024871826 - average: 2.1373575041453043; Accuracy: 0.2339\n",
      "Epoch 14 completed. Loss - total: 126938.76419830322 - average: 2.1156460699717203; Accuracy: 0.2429\n",
      "Epoch 15 completed. Loss - total: 125593.56092834473 - average: 2.0932260154724123; Accuracy: 0.25575\n",
      "Epoch 16 completed. Loss - total: 123791.63726043701 - average: 2.063193954340617; Accuracy: 0.27085\n",
      "Epoch 17 completed. Loss - total: 121957.3237915039 - average: 2.0326220631917318; Accuracy: 0.28651666666666664\n",
      "Epoch 18 completed. Loss - total: 119700.86004638672 - average: 1.9950143341064452; Accuracy: 0.3028166666666667\n",
      "Epoch 19 completed. Loss - total: 117410.00500488281 - average: 1.9568334167480468; Accuracy: 0.3221\n",
      "Epoch 20 completed. Loss - total: 115467.97054672241 - average: 1.924466175778707; Accuracy: 0.3351166666666667\n",
      "Epoch 21 completed. Loss - total: 112704.09924316406 - average: 1.8784016540527344; Accuracy: 0.35665\n",
      "Epoch 22 completed. Loss - total: 109503.38990020752 - average: 1.825056498336792; Accuracy: 0.37885\n",
      "Epoch 23 completed. Loss - total: 106239.41958999634 - average: 1.7706569931666056; Accuracy: 0.4029\n",
      "Epoch 24 completed. Loss - total: 102995.92991638184 - average: 1.7165988319396972; Accuracy: 0.42215\n",
      "Epoch 25 completed. Loss - total: 99280.04128265381 - average: 1.6546673547108968; Accuracy: 0.44731666666666664\n",
      "Epoch 26 completed. Loss - total: 95569.97821044922 - average: 1.5928329701741537; Accuracy: 0.47091666666666665\n",
      "Epoch 27 completed. Loss - total: 91135.52463150024 - average: 1.518925410525004; Accuracy: 0.5007333333333334\n",
      "Epoch 28 completed. Loss - total: 87028.18893432617 - average: 1.450469815572103; Accuracy: 0.5254333333333333\n",
      "Epoch 29 completed. Loss - total: 82776.85889816284 - average: 1.3796143149693807; Accuracy: 0.5529166666666666\n",
      "Epoch 30 completed. Loss - total: 78423.83633804321 - average: 1.3070639389673868; Accuracy: 0.5778166666666666\n",
      "Epoch 31 completed. Loss - total: 73464.31021118164 - average: 1.2244051701863607; Accuracy: 0.6084833333333334\n",
      "Epoch 32 completed. Loss - total: 68686.21830368042 - average: 1.1447703050613403; Accuracy: 0.6377833333333334\n",
      "Epoch 33 completed. Loss - total: 64725.82015991211 - average: 1.0787636693318685; Accuracy: 0.6591666666666667\n",
      "Epoch 34 completed. Loss - total: 59561.324363708496 - average: 0.9926887393951416; Accuracy: 0.6923666666666667\n",
      "Epoch 35 completed. Loss - total: 55072.84903717041 - average: 0.9178808172861735; Accuracy: 0.7201833333333333\n",
      "Epoch 36 completed. Loss - total: 50713.12060928345 - average: 0.8452186768213907; Accuracy: 0.7449166666666667\n",
      "Epoch 37 completed. Loss - total: 46883.386236190796 - average: 0.7813897706031799; Accuracy: 0.7657833333333334\n",
      "Epoch 38 completed. Loss - total: 42459.36376571655 - average: 0.7076560627619426; Accuracy: 0.7912833333333333\n",
      "Epoch 39 completed. Loss - total: 38702.916818618774 - average: 0.6450486136436462; Accuracy: 0.8126666666666666\n",
      "Epoch 40 completed. Loss - total: 35256.75187301636 - average: 0.5876125312169392; Accuracy: 0.8333833333333334\n",
      "Epoch 41 completed. Loss - total: 31690.62272644043 - average: 0.5281770454406738; Accuracy: 0.8543333333333333\n",
      "Epoch 42 completed. Loss - total: 28209.608083724976 - average: 0.47016013472874957; Accuracy: 0.8725833333333334\n",
      "Epoch 43 completed. Loss - total: 25561.983127593994 - average: 0.42603305212656656; Accuracy: 0.8878333333333334\n",
      "Epoch 44 completed. Loss - total: 23262.882336616516 - average: 0.38771470561027527; Accuracy: 0.8982833333333333\n",
      "Epoch 45 completed. Loss - total: 21402.95075893402 - average: 0.35671584598223366; Accuracy: 0.90845\n",
      "Epoch 46 completed. Loss - total: 18967.74419116974 - average: 0.31612906985282896; Accuracy: 0.9208833333333334\n",
      "Epoch 47 completed. Loss - total: 16850.009039878845 - average: 0.28083348399798075; Accuracy: 0.9304666666666667\n",
      "Epoch 48 completed. Loss - total: 15345.626054763794 - average: 0.2557604342460632; Accuracy: 0.9383833333333333\n",
      "Epoch 49 completed. Loss - total: 14762.495038986206 - average: 0.24604158398310344; Accuracy: 0.9399833333333333\n",
      "Epoch 50 completed. Loss - total: 12632.846603870392 - average: 0.21054744339783987; Accuracy: 0.94945\n",
      "Epoch 51 completed. Loss - total: 11961.704574584961 - average: 0.19936174290974934; Accuracy: 0.9520833333333333\n",
      "Epoch 52 completed. Loss - total: 10939.931104183197 - average: 0.18233218506971996; Accuracy: 0.9549666666666666\n",
      "Epoch 53 completed. Loss - total: 10533.71672487259 - average: 0.17556194541454315; Accuracy: 0.9573666666666667\n",
      "Epoch 54 completed. Loss - total: 9299.762363910675 - average: 0.15499603939851125; Accuracy: 0.9617\n",
      "Epoch 55 completed. Loss - total: 8988.284497737885 - average: 0.14980474162896473; Accuracy: 0.96295\n",
      "Epoch 56 completed. Loss - total: 8026.285751342773 - average: 0.13377142918904622; Accuracy: 0.96615\n",
      "Epoch 57 completed. Loss - total: 7655.4904935359955 - average: 0.12759150822559992; Accuracy: 0.9683333333333334\n",
      "Epoch 58 completed. Loss - total: 6766.045578718185 - average: 0.11276742631196976; Accuracy: 0.9717666666666667\n",
      "Epoch 59 completed. Loss - total: 6869.2709012031555 - average: 0.11448784835338592; Accuracy: 0.9707666666666667\n",
      "Epoch 60 completed. Loss - total: 6239.475657701492 - average: 0.10399126096169153; Accuracy: 0.9740833333333333\n",
      "Epoch 61 completed. Loss - total: 5931.896166086197 - average: 0.09886493610143661; Accuracy: 0.97505\n",
      "Epoch 62 completed. Loss - total: 5340.29168510437 - average: 0.08900486141840618; Accuracy: 0.9777333333333333\n",
      "Epoch 63 completed. Loss - total: 5221.420615196228 - average: 0.08702367691993713; Accuracy: 0.9783666666666667\n",
      "Epoch 64 completed. Loss - total: 4684.02649474144 - average: 0.07806710824569066; Accuracy: 0.9809166666666667\n",
      "Epoch 65 completed. Loss - total: 4931.917390584946 - average: 0.08219862317641576; Accuracy: 0.979\n",
      "Epoch 66 completed. Loss - total: 4132.56898188591 - average: 0.06887614969809851; Accuracy: 0.98375\n",
      "Epoch 67 completed. Loss - total: 3620.595940589905 - average: 0.06034326567649841; Accuracy: 0.9862166666666666\n",
      "Epoch 68 completed. Loss - total: 3430.688811302185 - average: 0.057178146855036416; Accuracy: 0.9866\n",
      "Epoch 69 completed. Loss - total: 3156.847821712494 - average: 0.0526141303618749; Accuracy: 0.9883833333333333\n",
      "Epoch 70 completed. Loss - total: 2996.343425154686 - average: 0.04993905708591143; Accuracy: 0.98875\n",
      "Epoch 71 completed. Loss - total: 2794.630529642105 - average: 0.04657717549403508; Accuracy: 0.9901333333333333\n",
      "Epoch 72 completed. Loss - total: 2579.141372561455 - average: 0.042985689542690914; Accuracy: 0.9908\n",
      "Epoch 73 completed. Loss - total: 2191.628147959709 - average: 0.03652713579932849; Accuracy: 0.9932166666666666\n",
      "Epoch 74 completed. Loss - total: 2324.66976583004 - average: 0.03874449609716733; Accuracy: 0.9921333333333333\n",
      "Epoch 75 completed. Loss - total: 2067.192252933979 - average: 0.034453204215566316; Accuracy: 0.99325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 completed. Loss - total: 1803.0588864088058 - average: 0.030050981440146764; Accuracy: 0.9948833333333333\n",
      "Epoch 77 completed. Loss - total: 1641.8966326713562 - average: 0.027364943877855936; Accuracy: 0.9952333333333333\n",
      "Epoch 78 completed. Loss - total: 1364.9683384895325 - average: 0.022749472308158873; Accuracy: 0.9968333333333333\n",
      "Epoch 79 completed. Loss - total: 1276.4597100615501 - average: 0.021274328501025834; Accuracy: 0.9972833333333333\n",
      "Epoch 80 completed. Loss - total: 1224.0443016290665 - average: 0.020400738360484443; Accuracy: 0.9972666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1224.0443016290665, 0.9972666666666666)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "learn_rate = 0.1 # for SGD\n",
    "num_epochs = 80\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = MLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(adam, step_size=5, gamma=.2)\n",
    "\n",
    "train_model(model, trainloader_permuted, loss_fn, optimizer, num_epochs, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
