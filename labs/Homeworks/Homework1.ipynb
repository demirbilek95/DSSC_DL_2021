{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recorded-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-veteran",
   "metadata": {},
   "source": [
    "#### Homework (see Moodle for additional info)\n",
    "\n",
    "1. build the MLP using PT built-ins as above\n",
    "2. Instantiate and summarise\n",
    "3. Provide calculation for the exact number of parameters of the MLP (also in the case of bias terms)\n",
    "4. Calculate the L1 and L2 norm of parameters for the params of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitting-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_New(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=5, out_features=11,bias=False)\n",
    "        self.layer2 = torch.nn.Linear(in_features=11, out_features=16,bias=False)\n",
    "        self.layer3 = torch.nn.Linear(in_features=16, out_features=13,bias=False)\n",
    "        self.layer4 = torch.nn.Linear(in_features=13, out_features=8,bias=False)\n",
    "        self.layer5 = torch.nn.Linear(in_features=8, out_features=4,bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layer1(X)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer5(out)\n",
    "        out = torch.nn.functional.softmax(out)\n",
    "        return out\n",
    "\n",
    "    def getListOfLayers(self):\n",
    "        return [self.layer1,self.layer2,self.layer3,self.layer4,self.layer5]\n",
    "\n",
    "    def printNormsOfLayers(self,norm_type):        \n",
    "        #list_of_layers = self.getListOfLayers()\n",
    "        print(\"L{norm_type} norm of layer1 is:\".format(norm_type,torch.norm(self.layer1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-interaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_New(\n",
       "  (layer1): Linear(in_features=5, out_features=11, bias=False)\n",
       "  (layer2): Linear(in_features=11, out_features=16, bias=False)\n",
       "  (layer3): Linear(in_features=16, out_features=13, bias=False)\n",
       "  (layer4): Linear(in_features=13, out_features=8, bias=False)\n",
       "  (layer5): Linear(in_features=8, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLP_New()\n",
    "mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greenhouse-bouquet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            55\n",
      "├─Linear: 1-2                            176\n",
      "├─Linear: 1-3                            208\n",
      "├─Linear: 1-4                            104\n",
      "├─Linear: 1-5                            32\n",
      "=================================================================\n",
      "Total params: 575\n",
      "Trainable params: 575\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Linear: 1-1                            55\n",
       "├─Linear: 1-2                            176\n",
       "├─Linear: 1-3                            208\n",
       "├─Linear: 1-4                            104\n",
       "├─Linear: 1-5                            32\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "answering-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters without bias:  575\n",
      "Number of parameters with bias:  627\n"
     ]
    }
   ],
   "source": [
    "# Below part is calculated by manually, hope it was the purpose of question\n",
    "print(\"Number of parameters without bias: \",5*11 + 11*16 + 16*13 + 13*8 + 8*4)\n",
    "print(\"Number of parameters with bias: \",5*11 + 11 + 11*16 + 16 + 16*13 + 13 + 13*8 + 8+ 8*4 + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-amazon",
   "metadata": {},
   "source": [
    "* Number of parameters without bias: $5 \\times 11 + 11 \\times 16 + 16 \\times 13 + 13 \\times 8 + 8 \\times 4 = 575$\n",
    "\n",
    "* Number of parameters with bias: $5 \\times 11 + 11 + 11 \\times 16 + 16 + 16 \\times 13 + 13 + 13 \\times 8 + 8 + 8 \\times 4 + 4 = 627$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "about-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight L1 norm of layer is:  tensor(3.1176) L2 norm of layer is:  tensor(1.4463)\n",
      "layer2.weight L1 norm of layer is:  tensor(3.0540) L2 norm of layer is:  tensor(1.1616)\n",
      "layer3.weight L1 norm of layer is:  tensor(2.3010) L2 norm of layer is:  tensor(1.0164)\n",
      "layer4.weight L1 norm of layer is:  tensor(1.3139) L2 norm of layer is:  tensor(0.9169)\n",
      "layer5.weight L1 norm of layer is:  tensor(0.7274) L2 norm of layer is:  tensor(0.6791)\n"
     ]
    }
   ],
   "source": [
    "model_layers_dict = mlp_model.state_dict()\n",
    "for layer_name, layer_weigths in model_layers_dict.items():\n",
    "    print(layer_name, \"L1 norm of layer is: \", torch.linalg.norm(layer_weigths,1), \"L2 norm of layer is: \", torch.linalg.norm(layer_weigths,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
